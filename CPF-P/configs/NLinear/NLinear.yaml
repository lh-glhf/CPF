# Basic Config
do_train: True  # Enable training
train_only: False  # Do both training and testing
task_id: test  # Task identifier for the experiment
seed: 2021  # Seed for reproducibility
model: 'NLinear'  # Model type (NLinear for linear forecasting)

# Data Loader Configuration
data: 'ETTm1'  # Dataset identifier (ETTm1 for multivariate time-series)
root_path: './data/ETT/'  # Root directory for dataset storage
data_path: 'ETTh1.csv'  # Path to the dataset file
features: 'M'  # Use multivariate features for forecasting
target: 'OT'  # Target variable to predict
freq: 'h'  # Frequency of the data (hourly)
checkpoints: './checkpoints/'  # Directory to save model checkpoints

# Forecasting Task
seq_len: 96  # Length of the input sequence (previous time steps)
label_len: 48  # Length of the label sequence (observed for prediction)
pred_len: 96  # Length of the prediction horizon (future steps)

# NLinear (Linear forecasting model)
individual: False  # Apply a shared linear model for all series

# Transformer and Formers Parameters (if applicable)
embed_type: 0  # Embedding type (0: no embedding)
enc_in: 7  # Number of input variables for encoder
dec_in: 7  # Number of input variables for decoder
c_out: 7  # Number of output variables (target dimensions)
d_model: 512  # Dimension of model hidden layers (for transformer modules)
n_heads: 8  # Number of attention heads
e_layers: 2  # Number of encoder layers
d_layers: 1  # Number of decoder layers
d_ff: 2048  # Dimension of feed-forward network
moving_avg: 25  # Moving average window size
factor: 1  # Factor for scaling in time-series
distil: True  # Use distillation for model compression
dropout: 0.05  # Dropout rate for regularization
embed: 'timeF'  # Type of time embedding used
activation: 'gelu'  # Activation function
output_attention: False  # Do not output attention weights

# Prediction Settings
do_predict: False  # Do not enter prediction mode

# Optimization
num_workers: 10  # Number of worker threads for data loading
itr: 1  # Number of iterations (or trials)
train_epochs: 10  # Number of epochs for training
batch_size: 32  # Batch size for training
patience: 3  # Early stopping patience
learning_rate: 0.0001  # Learning rate for optimizer
des: 'test'  # Description for the experiment
loss: 'mse'  # Loss function (Mean Squared Error)
lradj: 'type1'  # Learning rate adjustment strategy
use_amp: False  # Disable Automatic Mixed Precision (AMP)

# GPU Settings
use_gpu: True  # Enable GPU for training
gpu: 0  # GPU ID to use
use_multi_gpu: False  # Do not use multiple GPUs
devices: '0,1,2,3'  # Available GPU devices (in case multi-GPU is enabled)

# Additional Settings
test_flop: False  # Do not test the floating point operations (FLOP)

pre_process: ['RIN', 'BN']  # List of preprocessing techniques (e.g., RIN: Reversible Instance Normalization, BN: Batch Normalization)
post_process: ['RIN']  # List of postprocessing techniques applied after model output (e.g., RIN)