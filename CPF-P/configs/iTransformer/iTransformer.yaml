# Basic Config
do_train: True  # Flag to indicate whether to train the model
task_id: test  # Identifier for the specific task or experiment
model: iTransformer  # Model type, here it is iTransformer
seed: 2021  # Random seed for reproducibility

# Data Loader
data: custom  # Name of the dataset used for training
root_path: ./data/electricity/  # Root directory where the dataset is stored
data_path: electricity.csv  # Path to the CSV data file
features: M  # Type of features ('M' for multivariate)
target: OT  # Target variable to be predicted
freq: h  # Frequency of the data (e.g., 'h' for hourly)
checkpoints: ./checkpoints/  # Directory to save model checkpoints

# Forecasting Task
seq_len: 96  # Length of the input sequence for training
label_len: 48  # Length of the labeled sequence for guiding the forecast
pred_len: 96  # Length of the forecast output sequence

# Model Define
enc_in: 7  # Number of input variables for the encoder
dec_in: 7  # Number of input variables for the decoder
c_out: 7  # Number of output variables for forecasting
d_model: 512  # Dimension of the model representation
n_heads: 8  # Number of attention heads
e_layers: 2  # Number of encoder layers
d_layers: 1  # Number of decoder layers
d_ff: 2048  # Dimension of the feedforward network
moving_avg: 25  # Moving average window size for trend extraction
factor: 1  # Factor that controls the prediction length adjustment
distil: False  # Disable distillation for model efficiency
dropout: 0.1  # Dropout rate for regularization
embed: timeF  # Type of embedding ('timeF' indicates time feature embedding)
activation: gelu  # Activation function (GELU: Gaussian Error Linear Unit)
output_attention: False  # Whether to output attention weights for interpretability
do_predict: True  # Flag to indicate whether to run prediction

# Optimization
num_workers: 10  # Number of workers for loading the data
itr: 1  # Number of training iterations (repeats)
train_epochs: 3  # Number of epochs for training
batch_size: 128  # Batch size used during training
patience: 3  # Early stopping patience (number of epochs without improvement)
learning_rate: 0.0001  # Learning rate for the optimizer
des: test  # Description of the experiment
loss: MSE  # Loss function (MSE: Mean Squared Error)
lradj: type1  # Strategy for adjusting the learning rate during training
use_amp: False  # Flag to disable Automatic Mixed Precision (AMP) training

# GPU
use_gpu: True  # Flag to indicate whether to use GPU
gpu: 0  # GPU device ID to be used for training
use_multi_gpu: False  # Disable multi-GPU training
devices: '0,1,2,3'  # Specify available devices (but multi-GPU is disabled)

# iTransformer Specific
exp_name: MTSF  # Name of the experiment (Multivariate Time Series Forecasting)
channel_independence: False  # Whether to treat each channel independently
inverse: False  # Whether to inverse the predicted output back to the original scale
class_strategy: projection  # Strategy for the classification task (projection-based)
target_root_path: ./data/electricity/  # Root directory for target data
target_data_path: electricity.csv  # Path to the target data file
efficient_training: False  # Disable efficient training optimizations
use_norm: True  # Enable normalization of the data
partial_start_index: 0  # Index to start partial data training

pre_process: ['RIN', 'BN']  # List of preprocessing techniques (e.g., RIN: Reversible Instance Normalization, BN: Batch Normalization)
post_process: ['RIN']  # List of postprocessing techniques applied after model output (e.g., RIN)